{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO54SaZQolHSXye/xfD+rzc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilsilfverskiold/transformers-nlp-docs/blob/main/cook/fine-tune/fine_tune_encoder_custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ2sT75ENJOJ"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install -U huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oQFZj17XNXTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# import file - make sure you set the correct path in your Google Drive (this would be the file I'm importing)\n",
        "# my file has two fields I'll be using called 'text' and 'keywords'\n",
        "file_path = '/content/drive/My Drive/8500_rows_keywords.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "aO4GieHDNZIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# remove any null values for the 'keywords' field - will cause issues later if there are any\n",
        "df = df[df['keywords'].notnull()]\n",
        "\n",
        "# Split dataset into training and temp (15% - 7.5% each) (for validation and testing)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.15, random_state=42)\n",
        "\n",
        "# Split temp into validation and testing\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "kwXtST0bNlRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "reWPlVweN3wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataset dict with the train, validate and test set\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "# print the dict\n",
        "dataset_dict"
      ],
      "metadata": {
        "id": "XudCrcgwN6-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) map out some examples from the dataset - my fields I'm using are text and keywords (make sure you set your own)\n",
        "def show_samples(dataset, num_samples=10, seed=42):\n",
        "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
        "    for example in sample:\n",
        "        print(f\"\\n'>> Text: {example['text']}'\")\n",
        "        print(f\"'>> Keywords: {example['keywords']}'\")\n",
        "\n",
        "\n",
        "show_samples(dataset_dict)"
      ],
      "metadata": {
        "id": "OzVLeVgvOJPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# set the correct model you'll be fine-tuning (see seq-to-seq model docs for more information)\n",
        "model_name = 'facebook/bart-large'\n",
        "# get the tokenizer for the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# check the token length of the keywords field - you can do this for both fields\n",
        "texts = dataset_dict['train']['keywords']\n",
        "\n",
        "# Tokenize all texts and find the maximum length (max for BART is 1024 tokens)\n",
        "max_token_length = max(len(tokenizer.encode(text, truncation=True)) for text in texts)\n",
        "print(f\"The longest text is {max_token_length} tokens long.\")"
      ],
      "metadata": {
        "id": "Q6SDIn5UOQxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert both the input text and the target text into a format suitable for training a sequence-to-sequence model\n",
        "# remember data preprocessing functions would look different if you were using a model with a different architecture, such as an encoder-only or decoder-only model.\n",
        "\n",
        "def get_feature(batch):\n",
        "  encodings = tokenizer(batch['text'], text_target=batch['keywords'],\n",
        "                        max_length=1024, truncation=True)\n",
        "\n",
        "  encodings = {'input_ids': encodings['input_ids'],\n",
        "               'attention_mask': encodings['attention_mask'],\n",
        "               'labels': encodings['labels']}\n",
        "\n",
        "  return encodings\n",
        "\n",
        "dataset_pt = dataset_dict.map(get_feature, batched=True)"
      ],
      "metadata": {
        "id": "bCx5ZNx4PFwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset should be formatted as PyTorch tensors with only the new fields\n",
        "# i.e. specifies which columns should be returned when accessing the data - only the new fields will be returned\n",
        "columns = ['input_ids', 'labels', 'attention_mask']\n",
        "dataset_pt.set_format(type='torch', columns=columns)\n",
        "\n",
        "dataset_pt"
      ],
      "metadata": {
        "id": "b_P8wsZSPigN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the data collator is responsible for dynamically padding the batches to the maximum length in each batch.\n",
        "# which is crucial for efficient training of transformer models like BART or T5.\n",
        "# padding will look different depending on the type of model you use, you can see here that this one is specifically for seq-to-seq\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "O1SHSKYzPnXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# start training the model\n",
        "# we're using the Trainer API which abstracts away a lot of complexity\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = 'bart_tech_keywords',\n",
        "    num_train_epochs=3, # your choice\n",
        "    warmup_steps = 500,\n",
        "    per_device_train_batch_size=4, # keep a small batch size when working with a small GPU\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay = 0.01, # helps prevent overfitting\n",
        "    logging_steps = 10,\n",
        "    evaluation_strategy = 'steps',\n",
        "    eval_steps=50, # base this on the size of your dataset and number of training epochs\n",
        "    save_steps=1e6,\n",
        "    gradient_accumulation_steps=16 # running this on a small GPU\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, data_collator=data_collator,\n",
        "                  train_dataset = dataset_pt['train'], eval_dataset = dataset_pt['validation'])\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# note: the trainer loss should go down, the validation loss may fluctuate for each evaluation step but consistently increasing validation, while training is going down could be a sign of overfitting"
      ],
      "metadata": {
        "id": "VpXNJ_YJPsv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "trainer.save_model('tech-keywords-extractor')"
      ],
      "metadata": {
        "id": "7zG4qM04QfQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# test the model using Hugging Face's pipeline\n",
        "pipe = pipeline('summarization', model='tech-keywords-extractor')\n",
        "\n",
        "# test the first item in the test set to see how it does\n",
        "test_text=dataset_dict['test'][0]['text']\n",
        "keywords = dataset_dict['test'][0]['keywords']\n",
        "print(\"the text: \", text_test)\n",
        "print(\"generated keywords: \", pipe(test_text))\n",
        "print(\"orginal keywords : \",keywords)"
      ],
      "metadata": {
        "id": "SsnP83-qQkOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over the test set to generate 50 examples at once\n",
        "for i in range(0, 50):\n",
        "    text_test = dataset_dict['test'][i]['text']\n",
        "    keywords = dataset_dict['test'][i]['keywords']\n",
        "    print(\"text: \", text_test)\n",
        "    print(\"generated keywords: \", pipe(text_test)[0]['summary_text'])\n",
        "    print(\"original keywords: \", keywords)"
      ],
      "metadata": {
        "id": "B1J_AtcwRFxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you're satisfied we can push it to Hugging Face\n",
        "# you'll need a token from your Hugging Face account to log in\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "mFj9ixfdRcDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you would replace your own name here\n",
        "# you do not need to create a repository beforehand\n",
        "trainer.push_to_hub(\"ilsilfverskiold/tech-keywords-extractor\")"
      ],
      "metadata": {
        "id": "2K29RCIoRvRw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}